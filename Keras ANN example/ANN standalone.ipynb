{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5887d45c-be68-4c63-8480-fb5ed2ba090f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Stand-alone ANN version with per-epoch statistics: Experimental</b></div>\n",
    "Can be used for running overfitting tests or for general testing of new configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d8995c-c9e8-4195-9fe4-898bbae2f248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from itertools import cycle\n",
    "from tensorflow.keras import backend as K_backend\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tqdm.keras import TqdmCallback\n",
    "\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"   # Disable GPU if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe620e5-b2d8-405c-b5c2-cef05bd67cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noisy_duplicate_keras_b(data_array_loc): # alt version, might fix the memory issue\n",
    "    data_array_new = data_array_loc.copy()\n",
    "    # Element-by-element\n",
    "    for i in range(trainset_length):\n",
    "        for j in range(input_num_units):\n",
    "            data_array_new[i][j] = data_array_new[i][j]*(1 + noise_factor*(0.5 - np.random.random_sample()))\n",
    "    return data_array_new\n",
    "\n",
    "def hp_config_print(hp_list):\n",
    "    label_list = [\"Config: \", \"/\", \"/\", \" - \", \"/\", \"/\", \", B \", \", E/LR \", \"/\", \", EF/NF \", \"/\"]\n",
    "    for item, label in zip(hp_list, label_list): print(label, item, sep='', end='')\n",
    "    print()\n",
    "\n",
    "def trainset_generator(x_array, y_array, batch_size):\n",
    "    pairs = zip(x_array, y_array)\n",
    "    cycle_pairs = cycle(pairs)\n",
    "    while (True):\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        for _ in range(batch_size):\n",
    "            x, y = next(cycle_pairs)\n",
    "            x_batch.append(x)\n",
    "            y_batch.append(y)\n",
    "        yield np.array(x_batch), np.array(y_batch)\n",
    "\n",
    "def met_R2(y_true, y_pred):\n",
    "    SS_res = K_backend.sum(K_backend.square(y_true - y_pred))\n",
    "    SS_tot = K_backend.sum(K_backend.square(y_true - K_backend.mean(y_true)))\n",
    "    return (1 - SS_res/(SS_tot + K_backend.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9f42e6-b168-4185-9995-377074a181b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init\n",
    "if __name__ == '__main__':\n",
    "    print(\"Initialising...\")\n",
    "    work_path = os.path.join(\"k:\" + os.sep, \"Archive\", \"Studying\", \"NeuralNets\", \"Python\", \"Jupyter - Phycocyanin ANN\")\n",
    "    model_type = 1 # Sequential, Function API with one output layer, Function API with three output layers, extras\n",
    "    generator_enabled = True\n",
    "    low_priority_enabled = False\n",
    "    pickle_enabled = False\n",
    "    flag = 0\n",
    "    manual_valset_enabled = False\n",
    "    # Static parameters\n",
    "    dataset_length = 490 # Out of 490 total\n",
    "    train_list_length = dataset_length\n",
    "    input_num_units = 12 #\n",
    "    output_num_units = 3 # 3 is all\n",
    "    valset_length = 50 # 50\n",
    "    # Manually selected test dataset of 50 elements\n",
    "    if manual_valset_enabled:\n",
    "        test_points_list = [ 22,  28,  32,  41,  51,  55,  68,  73,  85,  91,\n",
    "                             96,  97, 109, 119, 124, 137, 150, 166, 173, 181,\n",
    "                            188, 193, 204, 214, 222, 241, 250, 259, 267, 280,\n",
    "                            292, 311, 319, 333, 342, 350, 361, 372, 378, 385,\n",
    "                            390, 394, 401, 408, 415, 435, 450, 461, 473, 482\n",
    "                           ]\n",
    "    else: test_points_list = np.random.randint(low=0, high=490, size=50).tolist()\n",
    "    # Hyperparameters\n",
    "    # Article hp_config: [10, 8, 9, 'sigmoid', 'sigmoid', 'sigmoid', 10, 5000, 0.001, 4, 0.05]\n",
    "    hyperparameters = [25, 25, 10, 'relu', 'sigmoid', 'sigmoid', 40, 10000, 0.001, 4, 0.05]\n",
    "    # Set RNG\n",
    "    seed = 128\n",
    "    #\n",
    "    print(\"Generator is \" + (\"enabled\" if generator_enabled else \"disabled\"))\n",
    "    print(\"Low process priority is \" + (\"enabled\" if low_priority_enabled else \"disabled\"))\n",
    "    print(\"pickle dumping is \" + (\"enabled\" if pickle_enabled else \"disabled\"))\n",
    "    print(\"Manual valset is \" + (\"enabled\" if manual_valset_enabled else \"disabled\"))\n",
    "    print(\"Model type is:\", model_type)\n",
    "    hp_config_print(hyperparameters)\n",
    "# End of __main__ section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd7fbb2-4d41-4eb7-af62-ee97f2b5b993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling dataset stuff\n",
    "if __name__ == '__main__':\n",
    "    if low_priority_enabled:\n",
    "        import psutil\n",
    "        this_process = psutil.Process(os.getpid())\n",
    "        this_process.nice(psutil.BELOW_NORMAL_PRIORITY_CLASS)\n",
    "    if pickle_enabled:\n",
    "        import pickle\n",
    "    rng = np.random.RandomState(seed)\n",
    "    tf.random.set_seed(seed) # Perhaps I need to set tf. seed separately?\n",
    "    # Reading data\n",
    "    df = pd.read_csv(os.path.join(work_path, \"Exp data SE 7 (good-504).csv\"))\n",
    "    #df.info()\n",
    "    df.drop(columns=['No', 'Init OD680', 'OD680'], inplace=True) # Drop the ID and 680s\n",
    "    df = df.replace(to_replace='???', value=np.NaN) # Replace '???'s with true NaNs for ease of processing\n",
    "    # Replace NaNs with means\n",
    "    for i in df.columns[df.isnull().any(axis=0)]:\n",
    "        df[i] = df[i].astype(np.float64) # Convert to float64 explicitly since there were string objects there\n",
    "        df[i].fillna(df[i].mean(),inplace=True)\n",
    "    mean_val = (df[df.columns.to_list()].mean()).to_numpy() # Not needed, testing only\n",
    "    max_val = (df[df.columns.to_list()].max()) # Divide wont' work with a np_array of max values\n",
    "    for i in df.columns:\n",
    "        df[i] = df[i].div(max_val[i]) # Divide one by one because if not, the columns will get sorted alphabetically\n",
    "    max_val = max_val.to_numpy()\n",
    "    # Valset\n",
    "    if True: # Are we doing the manual selected test set to imitate the main program behavior?\n",
    "        test_dataset = (df.loc[test_points_list]).to_numpy()\n",
    "        df.drop(test_points_list, axis=0, inplace=True)\n",
    "        train_list_length = dataset_length - valset_length\n",
    "    # Now, make it into a numpy array\n",
    "    dataset = df.to_numpy()\n",
    "    del df\n",
    "    #print(\"Max values are:\\n\", np.around(max_val, decimals=5))\n",
    "    #print(\"Mean values are:\\n\", np.around(mean_val, decimals=5))\n",
    "    #print(\"Processed dataset example:\\n\", dataset[55])\n",
    "    print(\"Dataset shape is:\", dataset.shape)\n",
    "    #\n",
    "    hidden1_num_units, hidden2_num_units, hidden3_num_units, hidden1_func, hidden2_func, hidden3_func, batch_size, epochs, learning_rate, extension_factor, noise_factor = hyperparameters\n",
    "    # Data normalization for the noise\n",
    "    dataset_norm = (dataset.copy())/(1 + noise_factor)  # Legacy\n",
    "    # Randomising data selection: grab the whole set and shuffle it\n",
    "    # I can't use Keras shuffle and validation split because of the inflation algorithm\n",
    "    # This way test set is different every time and randomly selected while still being X pre-selected numbers\n",
    "    trainset_length = train_list_length\n",
    "    train_in_array = np.zeros((trainset_length, input_num_units))\n",
    "    train_out_array = np.zeros((trainset_length, output_num_units))\n",
    "    #\n",
    "    val_in_array = np.zeros((valset_length, input_num_units))\n",
    "    val_out_array = np.zeros((valset_length, output_num_units))\n",
    "    #\n",
    "    #rand_list = np.random.choice(trainset_length, trainset_length, False)\n",
    "    for i in range(trainset_length):\n",
    "        line_to_add = dataset_norm[i]\n",
    "        train_in_array[i] = (line_to_add[:input_num_units].copy())/(1 + noise_factor)\n",
    "        train_out_array[i] = (line_to_add[input_num_units:].copy())\n",
    "    for i in range(valset_length):\n",
    "        line_to_add = test_dataset[i]\n",
    "        val_in_array[i] = (line_to_add[:input_num_units].copy())/(1 + noise_factor)\n",
    "        val_out_array[i] = (line_to_add[input_num_units:].copy())\n",
    "    # Extending dataset with 'noisy' duplicates\n",
    "    train_in_array_temp = train_in_array.copy()\n",
    "    train_out_array_temp = train_out_array.copy()\n",
    "    for i in range(0, extension_factor):\n",
    "        train_in_array_temp = np.concatenate((train_in_array_temp, noisy_duplicate_keras_b(train_in_array)))  # Noisy inputs\n",
    "        train_out_array_temp = np.concatenate((train_out_array_temp, train_out_array))                      # Clean corresponding outputs\n",
    "    train_in_array = train_in_array_temp.copy()    # Copy because I'm clearing it, which is probably unnecessary\n",
    "    train_out_array = train_out_array_temp.copy()\n",
    "    del train_in_array_temp\n",
    "    del train_out_array_temp\n",
    "    #\n",
    "    #train_list_length = train_list_length*(1 + extension_factor)   # Legacy\n",
    "    #if len(train_in_array) != train_list_length: print(\"--- Train list length is off! ---\")\n",
    "    # train_list = dataset[:dataset_length]\n",
    "    # test_list = dataset[dataset_length:]\n",
    "    print(\"Train input shape is:\", train_in_array.shape, \"train output shape is:\", train_out_array.shape)\n",
    "    print(\"Val input shape is:\", val_in_array.shape, \"val output shape is:\", val_out_array.shape)\n",
    "# End of __main__ section  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cda3b4e-0d05-43d7-b7bf-794928858fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a model for the set of hyperparameters to be used for a single stat loop\n",
    "if __name__ == '__main__':\n",
    "    if model_type == 0:\n",
    "        model = Sequential([\n",
    "            Dense(units=hidden1_num_units, input_shape=(input_num_units,), activation=hidden1_func),\n",
    "            Dense(units=hidden2_num_units, activation=hidden2_func),\n",
    "            Dense(units=hidden3_num_units, activation=hidden3_func),\n",
    "            Dense(units=output_num_units, activation='linear')\n",
    "        ])\n",
    "        model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse', metrics=[met_R2])\n",
    "    elif model_type == 1:\n",
    "        inputs = Input(shape=(input_num_units,))\n",
    "        h1 = Dense(units=hidden1_num_units, activation=hidden1_func)(inputs)\n",
    "        h2 = Dense(units=hidden2_num_units, activation=hidden2_func)(h1)\n",
    "        h3 = Dense(units=hidden3_num_units, activation=hidden3_func)(h2)\n",
    "        outputs = Dense(name='All', units=3, activation=\"linear\")(h3)\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=learning_rate), \n",
    "            loss={'All': 'mse'},\n",
    "            metrics={'All': met_R2}\n",
    "        )\n",
    "    elif model_type == 2:\n",
    "        inputs = Input(shape=(input_num_units,))\n",
    "        h1 = Dense(units=hidden1_num_units, activation=hidden1_func)(inputs)\n",
    "        h2 = Dense(units=hidden2_num_units, activation=hidden2_func)(h1)\n",
    "        h3 = Dense(units=hidden3_num_units, activation=hidden3_func)(h2)\n",
    "        outputs = [Dense(name='OD', units=1, activation=\"linear\")(h3), Dense(name='pH', units=1, activation=\"linear\")(h3), Dense(name='mrel', units=1, activation=\"linear\")(h3)]\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=learning_rate), \n",
    "            loss={'OD': 'mse', 'pH': 'mse', 'mrel': 'mse'},\n",
    "            metrics={'OD': met_R2, 'pH': met_R2, 'mrel': met_R2}\n",
    "        )\n",
    "    elif model_type == 3:\n",
    "        inputs = Input(shape=(input_num_units,))\n",
    "        h1 = Dense(units=hidden1_num_units, activation=hidden1_func)(inputs)\n",
    "        outputs = Dense(name='All', units=3, activation=\"linear\")(h1)\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=learning_rate), \n",
    "            loss={'All': 'mse'},\n",
    "            metrics={'All': met_R2}\n",
    "        )\n",
    "    elif model_type == 4:\n",
    "        inputs = Input(shape=(input_num_units,))\n",
    "        h1 = Dense(units=hidden1_num_units, activation=hidden1_func)(inputs)\n",
    "        h2 = Dense(units=hidden2_num_units, activation=hidden2_func)(h1)\n",
    "        outputs = Dense(name='All', units=3, activation=\"linear\")(h2)\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=learning_rate), \n",
    "            loss={'All': 'mse'},\n",
    "            metrics={'All': met_R2}\n",
    "        )\n",
    "    model.summary() # Debug    \n",
    "# End of __main__ section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07048648-6f8e-4d0d-a306-b7174149b433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the model to train\n",
    "if __name__ == '__main__':\n",
    "    # Fit: suffle=False because it is already shuffled because of how the dataset is split between train and test\n",
    "    print(\"Doing fit...\")\n",
    "    if generator_enabled:\n",
    "        trainset = trainset_generator(train_in_array, train_out_array, batch_size)\n",
    "        fit_history = model.fit(\n",
    "            trainset, validation_data=(val_in_array, val_out_array), steps_per_epoch=len(train_in_array)//batch_size, \n",
    "            batch_size=batch_size, epochs=epochs, shuffle=True, verbose=0, callbacks=[TqdmCallback(verbose=0), EarlyStopping(monitor='val_loss', mode='min', patience=1000)]\n",
    "        )\n",
    "    else:\n",
    "        if model_type in (2,):\n",
    "            train_out_array = [np.transpose(train_out_array)[0], np.transpose(train_out_array)[1], np.transpose(train_out_array)[2]]\n",
    "            val_out_array = [np.transpose(val_out_array)[0], np.transpose(val_out_array)[1], np.transpose(val_out_array)[2]]\n",
    "        fit_history = model.fit(\n",
    "            x=train_in_array, y=train_out_array, validation_data=(val_in_array, val_out_array), \n",
    "            batch_size=batch_size, epochs=epochs, shuffle=True, verbose=0, callbacks=[TqdmCallback(verbose=0), EarlyStopping(monitor='val_loss', mode='min', patience=1000)]\n",
    "        )\n",
    "    eval_results = model.evaluate(val_in_array, val_out_array, verbose=0)\n",
    "    print(\"Done!\")\n",
    "    if model_type in (0, 1, 3): print(\"Loss:\", \"{:.3f}\".format(eval_results[0]), \"\\nMetrics:\", \"{:.3f}\".format(eval_results[1]))\n",
    "    elif model_type in (2,): print(\"Loss:\", \"{:.3f}, {:.3f}, {:.3f}\".format(*eval_results[:3]), \"\\nMetrics:\", \"{:.3f}, {:.3f}, {:.3f}\".format(*eval_results[3:]))    \n",
    "    # No need to save the model here\n",
    "    # Now, grab the history to plot a learning curve\n",
    "    history_dict = fit_history.history\n",
    "    if pickle_enabled: # Outdated: I have to use pickle dump because the core dies if I try to plot with everythign else loaded\n",
    "        f = open(os.path.join(work_path, \"History_dump_temp\", 'wb'))\n",
    "        pickle.dump(history_dict, f, 2)\n",
    "        f.close\n",
    "        f = open(os.path.join(work_path, \"Epochs_dump_temp\", 'wb'))\n",
    "        pickle.dump(epochs, f, 2)\n",
    "        f.close\n",
    "        print(\"Dump done!\")\n",
    "# End of __main__ section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc37b60d-4fcf-489b-933d-9308c8f092fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotter functions\n",
    "def plot_fit_history(x_arr, y_fit, y_val, y_lim='F', met='Metrics', lang=0):\n",
    "    plt.figure(figsize=(16,10), dpi=100)\n",
    "    plt.plot(x_arr, y_fit, 'b-', label=(\"Training data\", \"Обучающие данные\")[lang])\n",
    "    plt.plot(x_arr, y_val, 'g-', label=(\"Validation data\", \"Валидационные данные\")[lang])\n",
    "    #plt.title((\"Learning curves: \", \"Кривые обучения: \")[lang] + met, fontsize=24)\n",
    "    plt.xlabel((\"Epochs\", \"Эпохи\")[lang], fontsize=20)\n",
    "    plt.ylabel(met, fontsize=20)\n",
    "    plt.legend(fontsize=16)\n",
    "    plt.grid()\n",
    "    if y_lim != 'F': plt.ylim(y_lim)\n",
    "    plt.show()\n",
    "\n",
    "def smooth_fit_history(value_arr, epochs, coeff):\n",
    "    smooth_arr = list()\n",
    "    for i in range(0, epochs, coeff): # Currently cuts off the 'tail'\n",
    "        smooth_arr.append(sum(value_arr[i:i+coeff])/coeff)\n",
    "    return smooth_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e4b407-0ba9-46cd-ae5d-93f28fee2c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotter for learning curves\n",
    "if __name__ == '__main__':\n",
    "    import matplotlib.pyplot as plt\n",
    "    if not 'pickle_enabled' in locals(): import pickle # If the var is not defined, the previous cell didn't run in this kernel, and as such we need to load from pickle\n",
    "    if pickle_enabled: \n",
    "        f = open(os.path.join(work_path, \"History_dump_temp\", 'rb'))\n",
    "        history_dict = pickle.load(f)\n",
    "        f.close\n",
    "        f = open(os.path.join(work_path, \"Epochs_dump_temp\", 'rb'))\n",
    "        epochs = pickle.load(f)\n",
    "        f.close\n",
    "    lang = 1 # English, Russian\n",
    "    graph_types = (('loss', ('MSE', \"Среднеквадратичная ошибка\")[lang], [0.0, 0.03]), ('met_R2', ('R2', \"Коэффициент детерминации\")[lang], [0.5, 1.0]))\n",
    "    sc = 10 # Smoothing coefficient\n",
    "    x_arr = list(range(0, epochs, sc))\n",
    "    if model_type in (0, 1, 3):\n",
    "        for item in graph_types:\n",
    "            plot_fit_history(x_arr, smooth_fit_history(history_dict[item[0]], epochs, sc), smooth_fit_history(history_dict['val_' + item[0]], epochs, sc), \n",
    "                             item[2], item[1], lang)\n",
    "    elif model_type in (2,):\n",
    "        history_avg = {'loss': [], 'met_R2': [], 'val_loss': [], 'val_met_R2': []}\n",
    "        for i in range(epochs): # Averaging across the outputs to get a similar kind of graph to models 0 and 1\n",
    "            for key in history_avg:\n",
    "                vh = 'val_' if 'val' in key else ''\n",
    "                kh = key[4:] if 'val' in key else key\n",
    "                history_avg[key].append((history_dict[vh + 'OD_' + kh][i] + history_dict[vh + 'pH_' + kh][i] + history_dict[vh + 'mrel_' + kh][i])/3)\n",
    "        print(\"Averaged across outputs graphs:\")\n",
    "        for item in graph_types:\n",
    "            plot_fit_history(x_arr, smooth_fit_history(history_avg[item[0]], epochs, sc), smooth_fit_history(history_avg['val_' + item[0]], epochs, sc), \n",
    "                             item[2], item[1], lang)\n",
    "        print(\"Individual output graphs:\")\n",
    "        for parameter in ('OD', 'pH', 'mrel'):\n",
    "            for item in graph_types:\n",
    "                key, label = (parameter + '_' + item[0], parameter + ': ' + item[1])\n",
    "                plot_fit_history(x_arr, smooth_fit_history(history_dict[key], epochs, sc), smooth_fit_history(history_dict['val_' + key], epochs, sc), \n",
    "                                 item[2], label, lang)\n",
    "# End of __main__ section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0134e45-7e10-44ca-ada4-ae1b2d1c7fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
